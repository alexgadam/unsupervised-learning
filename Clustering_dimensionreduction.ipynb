{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Clustering & Dimension Reduction\n",
    "\n",
    "### ASI Data Science\n",
    "\n",
    "#### Alexander Adam, Christiane Ahlheim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdbscan\n",
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from IPython.display import Image as display_image\n",
    "from scipy.cluster.vq import kmeans\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import hdbscan\n",
    "from umap import UMAP\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. k-means (application to image segmentation)\n",
    "\n",
    "In this first section, we introduce the k-means algorithm - the simplest clustering algorithm and its implementation in sklearn. As a simple application, we then show how k-means clustering can be used to segment an image by colour. By grouping similar (r,g,b) values together we should be able to extract different parts of the picture. We begin by loading an image below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imageio.imread('./Landscape_small.png')\n",
    "display_image(filename='./Landscape_small.png', width = 380, height = 380)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above consists of (314 x 500) pixels. Each entry in the img array above encodes the colour values for each pixel. There are four values - the three primary (rgb) colours as well as an alpha (opacity) value. The opacity value is not relevant here so we'll discard it. We need to reshape the img array above in other to get it into a form that is suitable for the sklearn machine learning algorithms. These generally take **two dimensional arrays** where **columns** are regarded as **features** and **rows** as **samples**. We'll therefore reshape img into a three column array (r,g,b) with as many rows as there are pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = img[:, :, :3]\n",
    "\n",
    "img_flat = sp.reshape(img, (img.shape[0]*img.shape[1],3)).astype(float)\n",
    "\n",
    "print(img.shape, img_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***K-means clustering in sklearn***  \n",
    "Notice the similarity in syntax to the classifiers used in supervised learning. kmeans requires the number of clusters as input. The fit method calculates the centroids based on the input data, whilst the predict method assigns cluster membership to each point. Notice that once fitted, the predict method is able to take new data - so you can assign cluster membership to new points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means in sklearn.\n",
    "clf = KMeans(n_clusters = 4, init='k-means++')\n",
    "clf.fit(img_flat.astype(float)) \n",
    "indices= clf.predict(img_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what these indices actually mean before moving on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the image back into 2D and pass back to plt.imshow()\n",
    "plt.imshow(sp.reshape(indices, (img.shape[0], img.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Identifying the number of clusters***\n",
    "\n",
    "The k-means algorithm requires the user to input the number of clusters. One issue is that in general we do not know a-priori what this value should be (unless we have application specific information). We should choose the number of clusters such that adding an additional cluster no longer gives 'better' modelling of the data. We can do this by plotting an objective function against the number of clusters and chosing the optimal value to be the point where the curve has a visible kink/elbow - indicating that adding additional clusters gives diminishing returns. A different approach is to optimise a quantity known as the average silhouette coefficient. This is something you can read about here:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "\n",
    "The code below shows how to generate such an 'elbow' plot using the average within-cluster sum of squares as the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids=[[] for i in range(1,10)]\n",
    "euclidean_distances=[0 for i in range(len(img_flat))]\n",
    "cluster_averaged_distances=[0 for i in range(1,10)]\n",
    "\n",
    "for K in range(1,10):\n",
    "    clf = KMeans(n_clusters = K, init='k-means++')\n",
    "    clf.fit(img_flat.astype(float))\n",
    "    \n",
    "    indices= clf.predict(img_flat)    \n",
    "    centroids= clf.cluster_centers_ \n",
    "    \n",
    "    for j in range(len(img_flat)):\n",
    "        euclidean_distances[j]=0\n",
    "        cluster=indices[j]\n",
    "        for k in range(3):\n",
    "            euclidean_distances[j]+=(img_flat[j][k] - centroids[cluster][k])**2\n",
    "        euclidean_distances[j]=(euclidean_distances[j])**0.5\n",
    "\n",
    "    cluster_averaged_distances[K-1]=np.mean(euclidean_distances)\n",
    "               \n",
    "plt.plot(range(1,10),cluster_averaged_distances)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "tt = plt.title('Example Elbow Curve')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, the loops in the calculation above make it very inefficient. There is a faster 'black box' implementation available in scipy (that uses a very slightly different objective function). The scipy method holds the results in a somewhat different format. It stores the coordinates of all centroids together with the average distance of points from their closest centroid (which is what we're after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â 2. HDBSCAN\n",
    "\n",
    "We have seen in this example that the k-means clustering algorithm performs reasonably well in our image example. However, in most scenarios it won't be as clear how many clusters you should expect, and you won't see an elbow in your plot to help you pick the right number of clusters.\n",
    "\n",
    "Look at the following plot:\n",
    "http://hdbscan.readthedocs.io/en/latest/_images/comparing_clustering_algorithms_6_0.png![image.png](attachment:image.png)\n",
    "\n",
    "Here, it is hard to tell how many clustes to expect.\n",
    "\n",
    "One of the most powerful clustering algorithms currently available is HDBSCAN: HDBSCAN is a density-based algorithm and performs single linkage clustering based on the density-space. \n",
    "HDBSCAN has two main advantages over k-means clustering:\n",
    "* We don't need to provide the algorithm with a number of clusters a-priori. Its parameters are interpretable.\n",
    "* HDBSCAN is conservative as it doesn't assign points to a cluster if there's no strong evidence to do so.\n",
    "\n",
    "Furthermore, the cluster-solution found by HDBSCAN is stable over subsampling the data and different parameter choices and can be efficiently implemented. \n",
    "\n",
    "\n",
    "You can find more info on HDBSCAN here: https://github.com/lmcinnes/umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For what follows it's useful to build a 2D scatter plot function that takes (data, labels) as arguments\n",
    "\n",
    "def scatter(x, colors):\n",
    "    count_colours = len(np.unique(colors))\n",
    "    palette = np.array(sns.color_palette(\"hls\", count_colours))\n",
    "\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
    "\n",
    "    return f, ax, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "n_samples = 1500\n",
    "blobs = datasets.make_blobs(n_samples=n_samples,\n",
    "                           random_state=42,\n",
    "                           centers=[[2, 2],\n",
    "                                    [-1, 1]],\n",
    "                           cluster_std=[0.1, 0.5])\n",
    "moons = datasets.make_moons(n_samples=n_samples, shuffle=True, noise=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = blobs[0] + moons[0] \n",
    "indices = np.zeros_like(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's have a look at our data!\n",
    "scatter(data, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise I\n",
    "Run k-means clustering and HDBSCAN clustering on the data. What results do you find and how do the two algorithms compare to each other? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Run k-means here (see above for code example)\"\n",
    "\n",
    "## scatter takes the original data and the predicted cluster indices from k-means as input.\n",
    "scatter(data, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Run HDBSCAN here (the syntax is similar to kmeans - see if you can work it out)\"\n",
    "\n",
    "scatter(data, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise II \n",
    "\n",
    "In the k-means case can you detect the optimal number of clusters using an elbow plot? Are you satisfied with the result? \n",
    "\n",
    "--> This illustrates how k-means can fail, even if we could visually detect the correct number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDBSCAN results makes more sense (and don't need our input to detect the number of clusters). \n",
    "Whilst k-means is an easy to understand algorithm, it has many downsides. HDBSCAN is able to detect structure in data and avoids common pitfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Digits and Dimension Reduction\n",
    "\n",
    "We now look at the sklearn digits dataset. These are images of hand-written digits. A classic problem (and now benchmmark for models) in machine learning is whether it is possible to train a classifier to distinguish between these different digits. This dataset is labelled but here we'll look at the extent to which purely unsupervised methods are able to segment this dataset. Each digit is an (8x8) collection of pixels and there are a total of 1797 samples in the dataset. This particular digits dataset can be regarded as a low dimensional analogue of the famous MNIST dataset. You can apply the exact same methods used here to that much higher dimensional case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "digits.data.shape\n",
    "\n",
    "X = np.vstack([digits.data[digits.target==i]\n",
    "               for i in range(10)])\n",
    "y = np.hstack([digits.target[digits.target==i]\n",
    "               for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about the details of this plotting function. It's simply the show the handwritten digits to you.\n",
    "nrows, ncols = 2, 5\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.gray()\n",
    "for i in range(ncols * nrows):\n",
    "    ax = plt.subplot(nrows, ncols, i + 1)\n",
    "    ax.matshow(digits.images[i,...])\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.title(digits.target[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Principal component analysis (PCA) in sklearn***\n",
    "\n",
    "The PCA algorithm requires you to specify the number of principal components that you wish to retain. Note that PCA has both fit() and transform() methods. The former calculates the SVD of the input data and the necessary coordinate transformation to reduce the desired number of dimensions, whilst the latter actually applies the transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=\"Number of dimensions to keep\")                    \n",
    "X_pca = pca.fit_transform(X)\n",
    "scatter(X_pca, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with clustering, it is important to have a mechanism to determine the correct number of principal components. In the case of PCA, the important quantity is the fraction of the total variance maintained as a function of the number of principal components retained. It is not uncommon to find in high dimensional dimension datasets that nearly all of the total variance can be retained whilst maintaining only a few principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"Re-run PCA, this time keeping all components. Use pca.explained_variance_ratio_ to plot\"\n",
    "\"the cumulative sum of variance retained as a function of principal components\"\n",
    "\n",
    "\"You code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a dimensional algorithm that projects data into a linear subspace. In contrast, there are other dimensional reduction algorithms that preserve *non-linear* structure. These algorithms can perform vastly better than PCA. The study of these falls under the topic of manifold learning. One particularly powerful example of manifold learning is an algorithm called t-SNE (t-distributed stochastic neighbour embedding). The details underpinning it are fairly involved and we won't talk about them here. You can read more about the algorithm here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "\n",
    "One downside of the t-SNE algorithm is that it takes a long time to train. A new and extremely powerful alternative is UMAP (Uniform Manifold Approximation and Projection). As you'll see if you do the exercise below, it is very dramatically able to separate the digits dataset. You can read more about it here: https://github.com/lmcinnes/umap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III\n",
    "\n",
    "Does PCA in 2D preserve the higher dimensional separation present in the digits dataset? How many components do you need to preserve this separation? Does UMAP preserve the separation? The syntax for umap is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_umap = UMAP(\n",
    "    n_neighbors=5,\n",
    "    min_dist=0.3,\n",
    "    metric='euclidean',\n",
    "    random_state=999\n",
    ").fit_transform(X)\n",
    "\n",
    "scatter(X_umap, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference to the results obtained by PCA are striking! We obtain 10 almost perfectly separated clusters. \n",
    "While this suggests that UMAP and other non-linear dimensionality reduction techniques may be superior to PCA and SVD, this might not always be true.\n",
    "In particular when the structure of the data is truly linear, non-linear algorithms can be misleading and their results are generally harder to interpret (you can see some examples of how t-SNE results deviate from the ground-truth here: https://distill.pub/2016/misread-tsne/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise IV\n",
    "\n",
    "Can you tune the parameters of an HDBSCAN clusterer so that it labels the UMAP-reduced data optimally? \n",
    "\n",
    "HDBSCAN has one main parameter, which is the minimum cluster size (min_cluster_size). There are others, but those are harder to interpret.\n",
    "In most cases, you won't be able to direclty assess how your clustering solution is doing. Here, we actually have access to the true labels (though we need to figure out first which cluster corresponds to which label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Cluster the dimensionally reduced UMAP results using HDBSCAN and plot\"\n",
    "scatter(X_umap, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other Sklearn Clustering Algorithms\n",
    "\n",
    "Clustering is an enormous subject and we have barely scratched the surface today. k-means (and its variations) are often a very good place to start, but in many practical situations it will be insufficient. This final section shows you some examples of where k-means fails to cluster correctly and invites you to investigate the behaviour of various other clustering algorithms in sklearn. We compare their ability to cluster four different datasets. The plotting function below will also give you some indication of the relative speeds of the different clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate four standard datasets for clustering. \n",
    "\n",
    "n_samples = 1500\n",
    "\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plotting function that will be used below. Do not worry about the details of this.\n",
    "\n",
    "def PlotClustering(clustering_names, clustering_algorithms):\n",
    "    \n",
    "    plt.figure(figsize=(len(clustering_names) * 2 + 3, 9.5))\n",
    "\n",
    "    plot_num = 1\n",
    "\n",
    "    datasets = [noisy_circles, noisy_moons, blobs, no_structure]\n",
    "    for i_dataset, dataset in enumerate(datasets):\n",
    "        X, y = dataset\n",
    "        X = StandardScaler().fit_transform(X) # NOTE: Normalise your data before clustering\n",
    "\n",
    "        for name, algorithm in zip(clustering_names, clustering_algorithms):\n",
    "            # Predict cluster memberships\n",
    "            plt.subplot(4, len(clustering_algorithms), plot_num)\n",
    "            if i_dataset == 0:\n",
    "                plt.title(name, size=18)\n",
    "\n",
    "            if algorithm is None:\n",
    "                plt.scatter(X[:, 0], X[:, 1], color='gray', s=10)\n",
    "            else:\n",
    "                t0 = time.time()\n",
    "                algorithm.fit(X)\n",
    "                t1 = time.time()\n",
    "                if hasattr(algorithm, 'labels_'):\n",
    "                    y_pred = algorithm.labels_.astype(np.int)\n",
    "                else:\n",
    "                    y_pred = algorithm.predict(X)\n",
    "\n",
    "                # Plot\n",
    "                plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n",
    "\n",
    "                if hasattr(algorithm, 'cluster_centers_'):\n",
    "                    centers = algorithm.cluster_centers_\n",
    "                    center_colors = colors[:len(centers)]\n",
    "                    plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
    "            plt.xlim(-2, 2)\n",
    "            plt.ylim(-2, 2)\n",
    "            plt.xticks(())\n",
    "            plt.yticks(())\n",
    "            if algorithm is not None:\n",
    "                plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                         transform=plt.gca().transAxes, size=15,\n",
    "                         horizontalalignment='right')\n",
    "            plot_num += 1\n",
    "\n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise V\n",
    "\n",
    "Test different sklearn clustering algorithms on the four datasets below. You'll find they have very different behaviours. Ultimately HDBSCAN is better than essentially every other choice in sklearn.\n",
    "\n",
    "http://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters = 2)\n",
    "\"You code goes here - add more sklearn clustering algorithms!\"\n",
    "\n",
    "names = ['Raw Data', 'kmeans']\n",
    "algorithms = [None, kmeans] \n",
    "\n",
    "PlotClustering(names, algorithms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
